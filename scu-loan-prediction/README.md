# ðŸ¦ Loan Grade & Diabetes Classification (SCU AI Competition â€“ Excellence Award)

This project was developed for the **2nd AI Competition hosted by Seoul Cyber University**.  
The competition involved solving two classification problems:

- **Loan Grade Prediction** (multi-class classification)  
- **Diabetes Prediction** (binary classification)

---

## ðŸ† Award  
**Excellence Award (ìš°ìˆ˜ìƒ)**  
Awarded based on the **average performance across both tasks.**

### ðŸ¥‡ Task 1: Loan Grade Prediction  
- [Leaderboard Link](https://www.kaggle.com/competitions/2-ai-loan/leaderboard)

### ðŸ¥ˆ Task 2: Diabetes Prediction  
- [Leaderboard Link](https://www.kaggle.com/competitions/2-ai/leaderboard)

---

## ðŸ” Problem Overview

The competition involved two separate tabular classification problems:

1. **Loan grade classification** based on customer financial information  
2. **Diabetes classification** based on medical and demographic data

The core challenge was to apply effective **preprocessing and feature engineering** strategies  
to improve model performance in a **structured data** setting.

---

## ðŸ“¦ Baseline Code Overview

A basic starter notebook was provided as a baseline, applying a `RandomForestClassifier`  
without significant preprocessing, feature engineering, or model tuning.

### ðŸ”¹ Baseline included:
- Dropping ID and label columns  
- Simple `train_test_split` (no cross-validation)  
- No missing value treatment  
- No engineered features  
- No hyperparameter tuning  
- Single model: `RandomForestClassifier`

While it served as a useful starting point, performance was limited by its simplicity.

ðŸ“„ [Baseline Notebook: ê³ ê°ëŒ€ì¶œë“±ê¸‰ ì™„ì „ ë² ì´ìŠ¤.ipynb](./ê³ ê°ëŒ€ì¶œë“±ê¸‰ ì™„ì „ ë² ì´ìŠ¤.ipynb)

---

## ðŸ§ª Experiment Timeline Summary

Through 40+ submissions, I conducted extensive experimentation in iterative phases.

### ðŸ”¹ Phase 1: Establishing a Baseline (Submissions #1â€“#3)
- Basic preprocessing (mean/median imputation)
- Initial attempts at feature creation  
- Minor performance gains observed

---

### ðŸ”¹ Phase 2: Feature Engineering & Selection (Submissions #4â€“#12)
- Introduced domain-informed features such as:  
  - `ì´ìžë¶€ë‹´ìœ¨` (interest burden ratio)  
  - `ê·¼ë¡œê¸°ê°„ ëŒ€ë¹„ ì‹ ìš©ê±°ëž˜ê¸°ê°„ ë¹„ìœ¨` (ratio of employment period to credit history length)  
- Applied scaling and encoding strategies  
- ðŸ”¥ Best single-feature result achieved using `ì´ìžë¶€ë‹´ìœ¨` (`0.6935`)

---

### ðŸ”¹ Phase 3: Model Exploration & Tuning (Submissions #6â€“#13)
- Compared multiple models: LightGBM, CatBoost, RandomForest, SVM  
- Eliminated underperformers  
- Tuned LightGBM and introduced ensemble methods  
- âœ… Best score in submission #13 using **LightGBM + Ensemble**

---

### ðŸ”¹ Phase 4: Feature Selection Refinement (Submissions #21â€“#30)
- Applied feature importance-based selection  
- Introduced ensemble voting/stacking with selected features  
- Used regularization to reduce overfitting

---

### ðŸ”¹ Phase 5: Post-peak Optimization (Submissions #31â€“#47)
- Tested feature interaction and reordering strategies  
- Conducted Optuna-based hyperparameter tuning  
- Some regressions observed â€” highlighting the importance of simplicity

Throughout, I emphasized **hypothesis-driven testing**, **reproducibility**, and **failure analysis**.

---

### ðŸ§  Task 2: Diabetes Classification

The second task involved predicting diabetes status using structured medical and demographic features.  
This dataset included missing values and **class imbalance**, which required careful handling.

ðŸ“ [Baseline Notebook (to be added)](./ë‹¹ë‡¨ë³‘ ì˜ˆì¸¡ ì´ì§„ë¶„ë¥˜(ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ).ipynb)

---

## ðŸ§ª Experiment Timeline Summary (Task 2)

Total of 40+ submissions, key stages:

### ðŸ”¹ Phase 1: Data Cleaning & Feature Addition (#1â€“#6)
- Handled missing values using domain logic  
  (e.g., gender-based imputation, mode value replacement)  
- Added custom features based on medical understanding  
  (e.g., derived variables from hemoglobin A1c)  
- âœ… Best improvement came from manual feature addition

---

### ðŸ”¹ Phase 2: Model Comparison (#7â€“#13)
- Evaluated LightGBM, CatBoost, RandomForest  
- LightGBM showed best results even with minimal tuning

---

### ðŸ”¹ Phase 3: Feature Engineering & Interaction (#14â€“#20)
- Introduced feature interactions such as:  
  - `[BMI Ã— í˜ˆë‹¹]`, `[ë‚˜ì´ Ã— ë‹¹í™”í˜ˆìƒ‰ì†Œ]`  
- Submission #18 yielded highest score: **0.811983** on Kaggle

---

### ðŸ”¹ Phase 4: Ensemble & Tuning (#21â€“#40)
- Applied:
  - Voting & Stacking  
  - Class weight balancing  
  - SMOTE for oversampling  
  - Optuna for hyperparameter search  
- F1 Macro peaked at **0.894** (validation), with Kaggle best at **0.811983**

---

## ðŸ¤– Models Used

| Model          | Used | Notes                              |
|----------------|------|------------------------------------|
| LightGBM       | âœ…    | Best performer; highly tunable    |
| CatBoost       | âœ…    | Stable results; used selectively  |
| XGBoost        | âœ…    | Strong in final tuning stages     |
| RandomForest   | âœ…    | Baseline only                     |
| SVM            | âŒ    | Low performance; discarded        |

---

## ðŸ’¡ Key Learnings

- Simple engineered features like `[ë‹¹í™”í˜ˆìƒ‰ì†Œ, í˜ˆë‹¹ì°¨ì´]` were highly predictive  
- Excessively complex pipelines sometimes degraded generalization  
- LightGBM + Feature Selection + Optuna formed the best-performing combo  
- Pseudo-labeling and SMOTE were only effective under certain conditions

> ðŸ“Œ This project taught me how **feature design, model selection, and ensemble strategies**  
> interact to shape the final performance in real-world tabular ML tasks.
